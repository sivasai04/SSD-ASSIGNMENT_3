{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251b38d5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # VidyaVichar Code Similarity Analysis\n",
    "# ## CS6.302 - Assignment 3, Question 1\n",
    "# \n",
    "# This notebook performs comprehensive similarity analysis across 27 MERN stack implementations\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part A: Preprocessing & Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193a3c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 projects to analyze...\n",
      "Processing 1/18: Team-26\n",
      "Processing 2/18: Team-03\n",
      "Processing 3/18: Team-27\n",
      "Processing 4/18: Team-33\n",
      "Processing 5/18: Team-28\n",
      "Processing 6/18: Team-06\n",
      "Processing 7/18: Team-29\n",
      "Processing 8/18: Team-31\n",
      "Processing 9/18: Team-16\n",
      "Processing 10/18: Team-22\n",
      "Processing 11/18: Team-25\n",
      "Processing 12/18: Team-05\n",
      "Processing 13/18: Team-20\n",
      "Processing 14/18: Team-10\n",
      "Processing 15/18: Team-14\n",
      "Processing 16/18: Team-21\n",
      "Processing 17/18: Team-13\n",
      "Processing 18/18: Team-32\n",
      "\n",
      "=== Project Summary Statistics ===\n",
      "Project  Total Files  Total Folders  Lines of Code  JS Files  JSX Files  JSON Files  CSS Files  React Components  Express Routes  Mongoose Models\n",
      "Team-26           29             27           4429        16          7           4          2                 0               5                4\n",
      "Team-03           32             30           7850        12         12           6          2                 0               2                2\n",
      "Team-27           53             35           5434        39         10           3          1                 0               7                3\n",
      "Team-33           46             32           2495        31          0           3         12                 0               4                3\n",
      "Team-28           37             30          21614        30          0           5          2                 0               3                3\n",
      "Team-06           44             30           7158        20         19           3          2                 0               2                4\n",
      "Team-29           36             31          20488        27          0           7          2                 0               2                3\n",
      "Team-31           23             28            924         9          6           2          6                 0               2                1\n",
      "Team-16           53             39           5182        17         26           8          2                 0               6                3\n",
      "Team-22           51             35          29544        44          0           5          2                 0               4                4\n",
      "Team-25           42             32          17943        17         20           4          1                 0               4                3\n",
      "Team-05           53             31          14200        16         29           4          4                 0               3                6\n",
      "Team-20           46             35          10202        24         16           5          1                 0               2                4\n",
      "Team-10           30             30          14749        14         10           4          2                 0               4                3\n",
      "Team-14           42             29          12468        20         11           4          7                 0               4                3\n",
      "Team-21           23             28          18741        20          0           3          0                 0               4                3\n",
      "Team-13         1129            406         178171       919         14         194          2                 0               4                5\n",
      "Team-32           32             25          21750        25          0           5          2                 0               3                3\n",
      "\n",
      "=== Summary Statistics ===\n",
      "       Total Files  Total Folders  Lines of Code    JS Files  JSX Files  \\\n",
      "count    18.000000      18.000000      18.000000   18.000000  18.000000   \n",
      "mean    100.055556      51.833333   21852.333333   72.222222  10.000000   \n",
      "std     256.982140      88.452214   39801.060630  211.525428   9.330532   \n",
      "min      23.000000      25.000000     924.000000    9.000000   0.000000   \n",
      "25%      32.000000      29.250000    5865.000000   16.250000   0.000000   \n",
      "50%      42.000000      30.500000   13334.000000   20.000000  10.000000   \n",
      "75%      49.750000      34.250000   20051.250000   29.250000  15.500000   \n",
      "max    1129.000000     406.000000  178171.000000  919.000000  29.000000   \n",
      "\n",
      "       JSON Files  CSS Files  React Components  Express Routes  \\\n",
      "count   18.000000  18.000000              18.0       18.000000   \n",
      "mean    14.944444   2.888889               0.0        3.611111   \n",
      "std     44.711457   2.846854               0.0        1.419979   \n",
      "min      2.000000   0.000000               0.0        2.000000   \n",
      "25%      3.250000   2.000000               0.0        2.250000   \n",
      "50%      4.000000   2.000000               0.0        4.000000   \n",
      "75%      5.000000   2.000000               0.0        4.000000   \n",
      "max    194.000000  12.000000               0.0        7.000000   \n",
      "\n",
      "       Mongoose Models  \n",
      "count        18.000000  \n",
      "mean          3.333333  \n",
      "std           1.084652  \n",
      "min           1.000000  \n",
      "25%           3.000000  \n",
      "50%           3.000000  \n",
      "75%           4.000000  \n",
      "max           6.000000  \n",
      "\n",
      "Preprocessing summary saved to results/preprocessing_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %%\n",
    "class CodePreprocessor:\n",
    "    \"\"\"Handles preprocessing of code files for similarity analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_dir: str):\n",
    "        self.projects_dir = Path(projects_dir)\n",
    "        self.valid_extensions = {'.js', '.jsx', '.json', '.css'}\n",
    "        self.projects_data = {}\n",
    "        \n",
    "    def remove_comments_and_logs(self, code: str, file_ext: str) -> str:\n",
    "        \"\"\"Remove comments and console logs from code\"\"\"\n",
    "        if file_ext in ['.js', '.jsx']:\n",
    "            # Remove single-line comments\n",
    "            code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "            # Remove multi-line comments\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "            # Remove console.log statements\n",
    "            code = re.sub(r'console\\.log\\([^)]*\\);?', '', code)\n",
    "        elif file_ext == '.css':\n",
    "            # Remove CSS comments\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "        return code\n",
    "    \n",
    "    def normalize_formatting(self, code: str) -> str:\n",
    "        \"\"\"Normalize whitespace and indentation\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        lines = [line.strip() for line in code.split('\\n')]\n",
    "        # Remove empty lines\n",
    "        lines = [line for line in lines if line]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def is_minified(self, code: str) -> bool:\n",
    "        \"\"\"Check if code is minified\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        if not lines:\n",
    "            return False\n",
    "        avg_line_length = sum(len(line) for line in lines) / len(lines)\n",
    "        return avg_line_length > 200\n",
    "    \n",
    "    def preprocess_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Preprocess a single file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            file_ext = file_path.suffix\n",
    "            \n",
    "            # Skip minified files\n",
    "            if self.is_minified(content):\n",
    "                return \"\"\n",
    "            \n",
    "            # Remove comments and logs\n",
    "            content = self.remove_comments_and_logs(content, file_ext)\n",
    "            \n",
    "            # Normalize formatting\n",
    "            content = self.normalize_formatting(content)\n",
    "            \n",
    "            return content\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_project(self, project_path: Path) -> Dict:\n",
    "        \"\"\"Analyze a single project and return statistics\"\"\"\n",
    "        stats = {\n",
    "            'project_name': project_path.name,\n",
    "            'total_files': 0,\n",
    "            'total_folders': 0,\n",
    "            'loc': 0,\n",
    "            'js_files': 0,\n",
    "            'jsx_files': 0,\n",
    "            'json_files': 0,\n",
    "            'css_files': 0,\n",
    "            'react_components': 0,\n",
    "            'express_routes': 0,\n",
    "            'mongoose_models': 0,\n",
    "            'file_contents': {}\n",
    "        }\n",
    "        \n",
    "        # Count folders\n",
    "        stats['total_folders'] = sum(1 for _ in project_path.rglob('*') if _.is_dir())\n",
    "        \n",
    "        # Process files\n",
    "        for file_path in project_path.rglob('*'):\n",
    "            if file_path.is_file() and file_path.suffix in self.valid_extensions:\n",
    "                stats['total_files'] += 1\n",
    "                \n",
    "                # Count by extension\n",
    "                ext = file_path.suffix\n",
    "                if ext == '.js':\n",
    "                    stats['js_files'] += 1\n",
    "                elif ext == '.jsx':\n",
    "                    stats['jsx_files'] += 1\n",
    "                elif ext == '.json':\n",
    "                    stats['json_files'] += 1\n",
    "                elif ext == '.css':\n",
    "                    stats['css_files'] += 1\n",
    "                \n",
    "                # Preprocess and store content\n",
    "                content = self.preprocess_file(file_path)\n",
    "                if content:\n",
    "                    relative_path = str(file_path.relative_to(project_path))\n",
    "                    stats['file_contents'][relative_path] = content\n",
    "                    \n",
    "                    # Count LOC\n",
    "                    stats['loc'] += len(content.split('\\n'))\n",
    "                    \n",
    "                    # Detect React components\n",
    "                    if ext in ['.js', '.jsx']:\n",
    "                        if re.search(r'(class\\s+\\w+\\s+extends\\s+React\\.Component|function\\s+\\w+\\s*\\([^)]*\\)\\s*{.*return\\s*\\(?\\s*<)', content):\n",
    "                            stats['react_components'] += 1\n",
    "                        \n",
    "                        # Detect Express routes\n",
    "                        if re.search(r'(app|router)\\.(get|post|put|delete|patch)\\s*\\(', content):\n",
    "                            stats['express_routes'] += 1\n",
    "                        \n",
    "                        # Detect Mongoose models\n",
    "                        if re.search(r'mongoose\\.model\\s*\\(|new\\s+mongoose\\.Schema\\s*\\(', content):\n",
    "                            stats['mongoose_models'] += 1\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def preprocess_all_projects(self) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess all projects and return summary DataFrame\"\"\"\n",
    "        all_stats = []\n",
    "        \n",
    "        # Get all project directories\n",
    "        project_dirs = [d for d in self.projects_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        print(f\"Found {len(project_dirs)} projects to analyze...\")\n",
    "        \n",
    "        for i, project_dir in enumerate(project_dirs, 1):\n",
    "            print(f\"Processing {i}/{len(project_dirs)}: {project_dir.name}\")\n",
    "            stats = self.analyze_project(project_dir)\n",
    "            self.projects_data[project_dir.name] = stats\n",
    "            all_stats.append({\n",
    "                'Project': stats['project_name'],\n",
    "                'Total Files': stats['total_files'],\n",
    "                'Total Folders': stats['total_folders'],\n",
    "                'Lines of Code': stats['loc'],\n",
    "                'JS Files': stats['js_files'],\n",
    "                'JSX Files': stats['jsx_files'],\n",
    "                'JSON Files': stats['json_files'],\n",
    "                'CSS Files': stats['css_files'],\n",
    "                'React Components': stats['react_components'],\n",
    "                'Express Routes': stats['express_routes'],\n",
    "                'Mongoose Models': stats['mongoose_models']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(all_stats)\n",
    "\n",
    "# %%\n",
    "# Initialize preprocessor\n",
    "# CHANGE THIS PATH to where your 27 projects are located\n",
    "PROJECTS_DIR = \"ALL_PROJECTS\"\n",
    "\n",
    "preprocessor = CodePreprocessor(PROJECTS_DIR)\n",
    "summary_df = preprocessor.preprocess_all_projects()\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "print(\"\\n=== Project Summary Statistics ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(summary_df.describe())\n",
    "\n",
    "# %%\n",
    "# Save preprocessing summary\n",
    "os.makedirs('results', exist_ok=True)\n",
    "summary_df.to_csv('results/preprocessing_summary.csv', index=False)\n",
    "print(\"\\nPreprocessing summary saved to results/preprocessing_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860447c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Part B: Code Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e54f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdifflib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequenceMatcher\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import itertools\n",
    "\n",
    "# %%\n",
    "class SimilarityAnalyzer:\n",
    "    \"\"\"Performs multi-level similarity analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_data: Dict):\n",
    "        self.projects_data = projects_data\n",
    "        self.project_names = list(projects_data.keys())\n",
    "        self.n_projects = len(self.project_names)\n",
    "        \n",
    "    def get_combined_content(self, project_name: str) -> str:\n",
    "        \"\"\"Combine all file contents for a project\"\"\"\n",
    "        contents = self.projects_data[project_name]['file_contents']\n",
    "        return '\\n\\n'.join(contents.values())\n",
    "    \n",
    "    def textual_similarity_difflib(self) -> np.ndarray:\n",
    "        \"\"\"Compute textual similarity using difflib\"\"\"\n",
    "        print(\"\\nComputing textual similarity (difflib)...\")\n",
    "        similarity_matrix = np.zeros((self.n_projects, self.n_projects))\n",
    "        \n",
    "        for i, proj1 in enumerate(self.project_names):\n",
    "            content1 = self.get_combined_content(proj1)\n",
    "            for j, proj2 in enumerate(self.project_names):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i][j] = 1.0\n",
    "                elif i < j:\n",
    "                    content2 = self.get_combined_content(proj2)\n",
    "                    ratio = SequenceMatcher(None, content1, content2).ratio()\n",
    "                    similarity_matrix[i][j] = ratio\n",
    "                    similarity_matrix[j][i] = ratio\n",
    "            print(f\"  Processed {i+1}/{self.n_projects}\")\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def textual_similarity_tfidf(self) -> np.ndarray:\n",
    "        \"\"\"Compute textual similarity using TF-IDF and cosine similarity\"\"\"\n",
    "        print(\"\\nComputing textual similarity (TF-IDF + Cosine)...\")\n",
    "        \n",
    "        # Prepare documents\n",
    "        documents = [self.get_combined_content(proj) for proj in self.project_names]\n",
    "        \n",
    "        # Create TF-IDF matrix\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def structural_similarity_ast(self) -> np.ndarray:\n",
    "        \"\"\"Compute structural similarity based on file structure and patterns\"\"\"\n",
    "        print(\"\\nComputing structural similarity...\")\n",
    "        similarity_matrix = np.zeros((self.n_projects, self.n_projects))\n",
    "        \n",
    "        def extract_structure_features(project_name: str) -> Dict:\n",
    "            \"\"\"Extract structural features from project\"\"\"\n",
    "            contents = self.projects_data[project_name]['file_contents']\n",
    "            features = {\n",
    "                'file_types': defaultdict(int),\n",
    "                'imports': set(),\n",
    "                'function_names': set(),\n",
    "                'route_patterns': set(),\n",
    "                'model_schemas': set()\n",
    "            }\n",
    "            \n",
    "            for path, content in contents.items():\n",
    "                ext = Path(path).suffix\n",
    "                features['file_types'][ext] += 1\n",
    "                \n",
    "                # Extract imports\n",
    "                imports = re.findall(r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', content)\n",
    "                features['imports'].update(imports)\n",
    "                \n",
    "                # Extract function names\n",
    "                functions = re.findall(r'(?:function|const|let|var)\\s+(\\w+)\\s*[=\\(]', content)\n",
    "                features['function_names'].update(functions)\n",
    "                \n",
    "                # Extract route patterns\n",
    "                routes = re.findall(r'\\.(get|post|put|delete|patch)\\s*\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]', content)\n",
    "                features['route_patterns'].update([r[1] for r in routes])\n",
    "                \n",
    "                # Extract schema fields\n",
    "                schemas = re.findall(r'(\\w+)\\s*:\\s*\\{\\s*type\\s*:', content)\n",
    "                features['model_schemas'].update(schemas)\n",
    "            \n",
    "            return features\n",
    "        \n",
    "        # Extract features for all projects\n",
    "        all_features = {proj: extract_structure_features(proj) for proj in self.project_names}\n",
    "        \n",
    "        # Compute similarity\n",
    "        for i, proj1 in enumerate(self.project_names):\n",
    "            feat1 = all_features[proj1]\n",
    "            for j, proj2 in enumerate(self.project_names):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i][j] = 1.0\n",
    "                elif i < j:\n",
    "                    feat2 = all_features[proj2]\n",
    "                    \n",
    "                    # Compute Jaccard similarity for various features\n",
    "                    similarities = []\n",
    "                    \n",
    "                    # Import similarity\n",
    "                    if feat1['imports'] or feat2['imports']:\n",
    "                        import_sim = len(feat1['imports'] & feat2['imports']) / len(feat1['imports'] | feat2['imports'])\n",
    "                        similarities.append(import_sim)\n",
    "                    \n",
    "                    # Function name similarity\n",
    "                    if feat1['function_names'] or feat2['function_names']:\n",
    "                        func_sim = len(feat1['function_names'] & feat2['function_names']) / len(feat1['function_names'] | feat2['function_names'])\n",
    "                        similarities.append(func_sim)\n",
    "                    \n",
    "                    # Route pattern similarity\n",
    "                    if feat1['route_patterns'] or feat2['route_patterns']:\n",
    "                        route_sim = len(feat1['route_patterns'] & feat2['route_patterns']) / len(feat1['route_patterns'] | feat2['route_patterns'])\n",
    "                        similarities.append(route_sim)\n",
    "                    \n",
    "                    # Model schema similarity\n",
    "                    if feat1['model_schemas'] or feat2['model_schemas']:\n",
    "                        schema_sim = len(feat1['model_schemas'] & feat2['model_schemas']) / len(feat1['model_schemas'] | feat2['model_schemas'])\n",
    "                        similarities.append(schema_sim)\n",
    "                    \n",
    "                    # Average similarity\n",
    "                    avg_sim = np.mean(similarities) if similarities else 0.0\n",
    "                    similarity_matrix[i][j] = avg_sim\n",
    "                    similarity_matrix[j][i] = avg_sim\n",
    "            \n",
    "            print(f\"  Processed {i+1}/{self.n_projects}\")\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def semantic_similarity_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Compute semantic similarity using sentence transformers\"\"\"\n",
    "        print(\"\\nComputing semantic similarity (embeddings)...\")\n",
    "        \n",
    "        try:\n",
    "            # Load model\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "            # Prepare documents (truncate to avoid memory issues)\n",
    "            documents = []\n",
    "            for proj in self.project_names:\n",
    "                content = self.get_combined_content(proj)\n",
    "                # Take first 10000 characters to avoid memory issues\n",
    "                documents.append(content[:10000])\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(documents, show_progress_bar=True)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            return similarity_matrix\n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic similarity: {e}\")\n",
    "            print(\"Falling back to TF-IDF similarity...\")\n",
    "            return self.textual_similarity_tfidf()\n",
    "\n",
    "# %%\n",
    "# Perform similarity analysis\n",
    "analyzer = SimilarityAnalyzer(preprocessor.projects_data)\n",
    "\n",
    "# Compute different similarity metrics\n",
    "similarity_difflib = analyzer.textual_similarity_difflib()\n",
    "similarity_tfidf = analyzer.textual_similarity_tfidf()\n",
    "similarity_structural = analyzer.structural_similarity_ast()\n",
    "similarity_semantic = analyzer.semantic_similarity_embeddings()\n",
    "\n",
    "# %%\n",
    "# Save similarity matrices\n",
    "np.save('results/similarity_difflib.npy', similarity_difflib)\n",
    "np.save('results/similarity_tfidf.npy', similarity_tfidf)\n",
    "np.save('results/similarity_structural.npy', similarity_structural)\n",
    "np.save('results/similarity_semantic.npy', similarity_semantic)\n",
    "\n",
    "print(\"\\nSimilarity matrices saved to results/\")\n",
    "\n",
    "# %%\n",
    "# Create DataFrames for better visualization\n",
    "project_names = analyzer.project_names\n",
    "\n",
    "df_difflib = pd.DataFrame(similarity_difflib, index=project_names, columns=project_names)\n",
    "df_tfidf = pd.DataFrame(similarity_tfidf, index=project_names, columns=project_names)\n",
    "df_structural = pd.DataFrame(similarity_structural, index=project_names, columns=project_names)\n",
    "df_semantic = pd.DataFrame(similarity_semantic, index=project_names, columns=project_names)\n",
    "\n",
    "# Save as CSV\n",
    "df_difflib.to_csv('results/similarity_matrix_difflib.csv')\n",
    "df_tfidf.to_csv('results/similarity_matrix_tfidf.csv')\n",
    "df_structural.to_csv('results/similarity_matrix_structural.csv')\n",
    "df_semantic.to_csv('results/similarity_matrix_semantic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3507b4",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Part C: Visualization & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45960fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# %%\n",
    "class SimilarityVisualizer:\n",
    "    \"\"\"Visualizes similarity analysis results\"\"\"\n",
    "    \n",
    "    def __init__(self, project_names: List[str]):\n",
    "        self.project_names = project_names\n",
    "        \n",
    "    def plot_heatmap(self, similarity_matrix: np.ndarray, title: str, filename: str):\n",
    "        \"\"\"Plot similarity heatmap\"\"\"\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        sns.heatmap(\n",
    "            similarity_matrix,\n",
    "            xticklabels=self.project_names,\n",
    "            yticklabels=self.project_names,\n",
    "            annot=False,\n",
    "            cmap='YlOrRd',\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            cbar_kws={'label': 'Similarity Score'}\n",
    "        )\n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Projects', fontsize=12)\n",
    "        plt.ylabel('Projects', fontsize=12)\n",
    "        plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "        plt.yticks(rotation=0, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_network_graph(self, similarity_matrix: np.ndarray, title: str, filename: str, threshold: float = 0.5):\n",
    "        \"\"\"Plot network graph of similar projects\"\"\"\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for name in self.project_names:\n",
    "            G.add_node(name)\n",
    "        \n",
    "        # Add edges for similarities above threshold\n",
    "        for i in range(len(self.project_names)):\n",
    "            for j in range(i + 1, len(self.project_names)):\n",
    "                if similarity_matrix[i][j] >= threshold:\n",
    "                    G.add_edge(\n",
    "                        self.project_names[i],\n",
    "                        self.project_names[j],\n",
    "                        weight=similarity_matrix[i][j]\n",
    "                    )\n",
    "        \n",
    "        # Layout\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue', alpha=0.9)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "        \n",
    "        # Draw edges with varying thickness\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] for u, v in edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], alpha=0.5)\n",
    "        \n",
    "        plt.title(f'{title}\\n(Threshold: {threshold})', fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_bar_chart(self, matrices: Dict[str, np.ndarray], filename: str):\n",
    "        \"\"\"Plot bar chart of average similarities\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        metric_names = list(matrices.keys())\n",
    "        avg_similarities = []\n",
    "        \n",
    "        for matrix in matrices.values():\n",
    "            # Get upper triangle (excluding diagonal)\n",
    "            mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "            avg_sim = matrix[mask].mean()\n",
    "            avg_similarities.append(avg_sim)\n",
    "        \n",
    "        bars = plt.bar(metric_names, avg_similarities, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
    "        plt.ylabel('Average Similarity Score', fontsize=12)\n",
    "        plt.xlabel('Similarity Metric', fontsize=12)\n",
    "        plt.title('Average Similarity Scores by Metric', fontsize=16, fontweight='bold')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, avg_similarities):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_distribution(self, similarity_matrix: np.ndarray, title: str, filename: str):\n",
    "        \"\"\"Plot distribution of similarity scores\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Get upper triangle (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "        similarities = similarity_matrix[mask]\n",
    "        \n",
    "        plt.hist(similarities, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(similarities.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {similarities.mean():.3f}')\n",
    "        plt.axvline(np.median(similarities), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(similarities):.3f}')\n",
    "        \n",
    "        plt.xlabel('Similarity Score', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: results/{filename}\")\n",
    "\n",
    "# %%\n",
    "# Create visualizations\n",
    "visualizer = SimilarityVisualizer(project_names)\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# Heatmaps\n",
    "visualizer.plot_heatmap(similarity_difflib, 'Textual Similarity (Difflib)', 'heatmap_difflib.png')\n",
    "visualizer.plot_heatmap(similarity_tfidf, 'Textual Similarity (TF-IDF)', 'heatmap_tfidf.png')\n",
    "visualizer.plot_heatmap(similarity_structural, 'Structural Similarity', 'heatmap_structural.png')\n",
    "visualizer.plot_heatmap(similarity_semantic, 'Semantic Similarity', 'heatmap_semantic.png')\n",
    "\n",
    "# Network graphs\n",
    "visualizer.plot_network_graph(similarity_tfidf, 'Project Similarity Network (TF-IDF)', 'network_tfidf.png', threshold=0.3)\n",
    "visualizer.plot_network_graph(similarity_structural, 'Project Similarity Network (Structural)', 'network_structural.png', threshold=0.3)\n",
    "\n",
    "# Bar chart\n",
    "matrices_dict = {\n",
    "    'Difflib': similarity_difflib,\n",
    "    'TF-IDF': similarity_tfidf,\n",
    "    'Structural': similarity_structural,\n",
    "    'Semantic': similarity_semantic\n",
    "}\n",
    "visualizer.plot_bar_chart(matrices_dict, 'average_similarities.png')\n",
    "\n",
    "# Distributions\n",
    "visualizer.plot_distribution(similarity_tfidf, 'Distribution of TF-IDF Similarity Scores', 'distribution_tfidf.png')\n",
    "visualizer.plot_distribution(similarity_structural, 'Distribution of Structural Similarity Scores', 'distribution_structural.png')\n",
    "\n",
    "# %%\n",
    "# Find most and least similar pairs\n",
    "def find_extreme_pairs(similarity_matrix: np.ndarray, project_names: List[str], metric_name: str):\n",
    "    \"\"\"Find most and least similar project pairs\"\"\"\n",
    "    n = len(project_names)\n",
    "    \n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pairs.append((project_names[i], project_names[j], similarity_matrix[i][j]))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    pairs.sorted = sorted(pairs, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(\"\\nTop 5 Most Similar Pairs:\")\n",
    "    for proj1, proj2, sim in pairs[:5]:\n",
    "        print(f\"  {proj1} <-> {proj2}: {sim:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 Least Similar Pairs:\")\n",
    "    for proj1, proj2, sim in pairs[-5:]:\n",
    "        print(f\"  {proj1} <-> {proj2}: {sim:.4f}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMILARITY ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "find_extreme_pairs(similarity_difflib, project_names, \"Textual Similarity (Difflib)\")\n",
    "find_extreme_pairs(similarity_tfidf, project_names, \"Textual Similarity (TF-IDF)\")\n",
    "find_extreme_pairs(similarity_structural, project_names, \"Structural Similarity\")\n",
    "find_extreme_pairs(similarity_semantic, project_names, \"Semantic Similarity\")\n",
    "\n",
    "# %%\n",
    "# Generate summary statistics\n",
    "summary_stats = {\n",
    "    'Metric': [],\n",
    "    'Mean': [],\n",
    "    'Median': [],\n",
    "    'Std Dev': [],\n",
    "    'Min': [],\n",
    "    'Max': []\n",
    "}\n",
    "\n",
    "for metric_name, matrix in matrices_dict.items():\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "    values = matrix[mask]\n",
    "    \n",
    "    summary_stats['Metric'].append(metric_name)\n",
    "    summary_stats['Mean'].append(values.mean())\n",
    "    summary_stats['Median'].append(np.median(values))\n",
    "    summary_stats['Std Dev'].append(values.std())\n",
    "    summary_stats['Min'].append(values.min())\n",
    "    summary_stats['Max'].append(values.max())\n",
    "\n",
    "stats_df = pd.DataFrame(summary_stats)\n",
    "stats_df.to_csv('results/summary_statistics.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nâœ… All analysis complete! Check the 'results/' folder for outputs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
