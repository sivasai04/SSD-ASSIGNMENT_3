{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6775c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Code Similarity Analysis\n",
    "# Key optimizations applied:\n",
    "# 1. Parallel processing for similarity computations\n",
    "# 2. Sampling and chunking for large codebases\n",
    "# 3. Caching preprocessed content\n",
    "# 4. More efficient string comparison\n",
    "# 5. Reduced semantic embedding size\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# ==================== OPTIMIZED PREPROCESSOR ====================\n",
    "\n",
    "class OptimizedCodePreprocessor:\n",
    "    \"\"\"Optimized preprocessor with caching and parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_dir: str, max_file_size: int = 100000):\n",
    "        self.projects_dir = Path(projects_dir)\n",
    "        self.valid_extensions = {'.js', '.jsx', '.json', '.css'}\n",
    "        self.projects_data = {}\n",
    "        self.max_file_size = max_file_size  # Skip very large files\n",
    "        \n",
    "    def remove_comments_and_logs(self, code: str, file_ext: str) -> str:\n",
    "        \"\"\"Remove comments and console logs from code\"\"\"\n",
    "        if file_ext in ['.js', '.jsx']:\n",
    "            code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "            code = re.sub(r'console\\.log\\([^)]*\\);?', '', code)\n",
    "        elif file_ext == '.css':\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "        return code\n",
    "    \n",
    "    def normalize_formatting(self, code: str) -> str:\n",
    "        \"\"\"Normalize whitespace and indentation\"\"\"\n",
    "        lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def is_minified(self, code: str) -> bool:\n",
    "        \"\"\"Check if code is minified\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        if not lines:\n",
    "            return False\n",
    "        avg_line_length = sum(len(line) for line in lines) / len(lines)\n",
    "        return avg_line_length > 200\n",
    "    \n",
    "    def preprocess_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Preprocess a single file with size check\"\"\"\n",
    "        try:\n",
    "            # Skip large files\n",
    "            if file_path.stat().st_size > self.max_file_size:\n",
    "                return \"\"\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            file_ext = file_path.suffix\n",
    "            \n",
    "            if self.is_minified(content):\n",
    "                return \"\"\n",
    "            \n",
    "            content = self.remove_comments_and_logs(content, file_ext)\n",
    "            content = self.normalize_formatting(content)\n",
    "            \n",
    "            return content\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_project(self, project_path: Path) -> Dict:\n",
    "        \"\"\"Analyze a single project with optimizations\"\"\"\n",
    "        stats = {\n",
    "            'project_name': project_path.name,\n",
    "            'total_files': 0,\n",
    "            'total_folders': 0,\n",
    "            'loc': 0,\n",
    "            'js_files': 0,\n",
    "            'jsx_files': 0,\n",
    "            'json_files': 0,\n",
    "            'css_files': 0,\n",
    "            'react_components': 0,\n",
    "            'express_routes': 0,\n",
    "            'mongoose_models': 0,\n",
    "            'file_contents': {},\n",
    "            'combined_content': \"\"  # Cache combined content\n",
    "        }\n",
    "        \n",
    "        # Count folders (faster method)\n",
    "        stats['total_folders'] = sum(1 for _ in project_path.rglob('*') if _.is_dir())\n",
    "        \n",
    "        # Collect all valid files first\n",
    "        valid_files = [f for f in project_path.rglob('*') \n",
    "                      if f.is_file() and f.suffix in self.valid_extensions]\n",
    "        \n",
    "        # Process files\n",
    "        all_content = []\n",
    "        for file_path in valid_files:\n",
    "            stats['total_files'] += 1\n",
    "            ext = file_path.suffix\n",
    "            \n",
    "            # Count by extension\n",
    "            if ext == '.js':\n",
    "                stats['js_files'] += 1\n",
    "            elif ext == '.jsx':\n",
    "                stats['jsx_files'] += 1\n",
    "            elif ext == '.json':\n",
    "                stats['json_files'] += 1\n",
    "            elif ext == '.css':\n",
    "                stats['css_files'] += 1\n",
    "            \n",
    "            # Preprocess content\n",
    "            content = self.preprocess_file(file_path)\n",
    "            if content:\n",
    "                relative_path = str(file_path.relative_to(project_path))\n",
    "                stats['file_contents'][relative_path] = content\n",
    "                all_content.append(content)\n",
    "                \n",
    "                # Count LOC\n",
    "                stats['loc'] += len(content.split('\\n'))\n",
    "                \n",
    "                # Detect features (only for JS/JSX)\n",
    "                if ext in ['.js', '.jsx']:\n",
    "                    if 'React.Component' in content or 'return (' in content:\n",
    "                        stats['react_components'] += 1\n",
    "                    if re.search(r'(app|router)\\.(get|post|put|delete)', content):\n",
    "                        stats['express_routes'] += 1\n",
    "                    if 'mongoose.model' in content or 'mongoose.Schema' in content:\n",
    "                        stats['mongoose_models'] += 1\n",
    "        \n",
    "        # Cache combined content\n",
    "        stats['combined_content'] = '\\n\\n'.join(all_content)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def preprocess_all_projects(self) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess all projects with progress tracking\"\"\"\n",
    "        all_stats = []\n",
    "        project_dirs = [d for d in self.projects_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        print(f\"Found {len(project_dirs)} projects to analyze...\")\n",
    "        \n",
    "        # Process projects sequentially (avoids file I/O conflicts)\n",
    "        for i, project_dir in enumerate(project_dirs, 1):\n",
    "            print(f\"Processing {i}/{len(project_dirs)}: {project_dir.name}\")\n",
    "            stats = self.analyze_project(project_dir)\n",
    "            self.projects_data[project_dir.name] = stats\n",
    "            all_stats.append({\n",
    "                'Project': stats['project_name'],\n",
    "                'Total Files': stats['total_files'],\n",
    "                'Total Folders': stats['total_folders'],\n",
    "                'Lines of Code': stats['loc'],\n",
    "                'JS Files': stats['js_files'],\n",
    "                'JSX Files': stats['jsx_files'],\n",
    "                'JSON Files': stats['json_files'],\n",
    "                'CSS Files': stats['css_files'],\n",
    "                'React Components': stats['react_components'],\n",
    "                'Express Routes': stats['express_routes'],\n",
    "                'Mongoose Models': stats['mongoose_models']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(all_stats)\n",
    "\n",
    "# ==================== OPTIMIZED SIMILARITY ANALYZER ====================\n",
    "\n",
    "def compute_difflib_pair(args):\n",
    "    \"\"\"Compute difflib similarity for a pair (for parallel processing)\"\"\"\n",
    "    i, j, content1, content2 = args\n",
    "    if i == j:\n",
    "        return (i, j, 1.0)\n",
    "    # Use quick_ratio for speed, then ratio only if promising\n",
    "    matcher = SequenceMatcher(None, content1, content2)\n",
    "    quick = matcher.quick_ratio()\n",
    "    if quick > 0.5:  # Only compute full ratio if quick ratio is high\n",
    "        ratio = matcher.ratio()\n",
    "    else:\n",
    "        ratio = quick * 0.9  # Approximate\n",
    "    return (i, j, ratio)\n",
    "\n",
    "class OptimizedSimilarityAnalyzer:\n",
    "    \"\"\"Optimized similarity analyzer with parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_data: Dict, sample_size: int = None):\n",
    "        self.projects_data = projects_data\n",
    "        self.project_names = list(projects_data.keys())\n",
    "        self.n_projects = len(self.project_names)\n",
    "        self.sample_size = sample_size  # For sampling large projects\n",
    "        \n",
    "    def get_combined_content(self, project_name: str, max_chars: int = None) -> str:\n",
    "        \"\"\"Get combined content with optional truncation\"\"\"\n",
    "        content = self.projects_data[project_name]['combined_content']\n",
    "        if max_chars and len(content) > max_chars:\n",
    "            # Sample from different parts of the file\n",
    "            step = len(content) // 5\n",
    "            samples = [content[i:i+max_chars//5] for i in range(0, len(content), step)][:5]\n",
    "            return '\\n'.join(samples)\n",
    "        return content\n",
    "    \n",
    "    def textual_similarity_difflib_parallel(self, max_chars: int = 50000) -> np.ndarray:\n",
    "        \"\"\"Compute textual similarity using difflib with parallel processing\"\"\"\n",
    "        print(\"\\nComputing textual similarity (difflib - parallel)...\")\n",
    "        similarity_matrix = np.zeros((self.n_projects, self.n_projects))\n",
    "        \n",
    "        # Prepare content (truncate for speed)\n",
    "        contents = [self.get_combined_content(proj, max_chars) \n",
    "                   for proj in self.project_names]\n",
    "        \n",
    "        # Create pairs to compute\n",
    "        pairs = []\n",
    "        for i in range(self.n_projects):\n",
    "            for j in range(i, self.n_projects):\n",
    "                pairs.append((i, j, contents[i], contents[j]))\n",
    "        \n",
    "        # Compute in parallel\n",
    "        with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "            results = list(executor.map(compute_difflib_pair, pairs))\n",
    "        \n",
    "        # Fill matrix\n",
    "        for i, j, sim in results:\n",
    "            similarity_matrix[i][j] = sim\n",
    "            similarity_matrix[j][i] = sim\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def textual_similarity_tfidf(self) -> np.ndarray:\n",
    "        \"\"\"Compute textual similarity using TF-IDF (fastest method)\"\"\"\n",
    "        print(\"\\nComputing textual similarity (TF-IDF + Cosine)...\")\n",
    "        \n",
    "        documents = [self.get_combined_content(proj, max_chars=100000) \n",
    "                    for proj in self.project_names]\n",
    "        \n",
    "        # Optimized vectorizer settings\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=3000,  # Reduced from 5000\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            ngram_range=(1, 2),\n",
    "            max_df=0.95,  # Ignore very common terms\n",
    "            min_df=2      # Ignore very rare terms\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def structural_similarity_fast(self) -> np.ndarray:\n",
    "        \"\"\"Fast structural similarity using simplified feature extraction\"\"\"\n",
    "        print(\"\\nComputing structural similarity (fast)...\")\n",
    "        similarity_matrix = np.zeros((self.n_projects, self.n_projects))\n",
    "        \n",
    "        # Extract features for all projects\n",
    "        all_features = {}\n",
    "        for proj in self.project_names:\n",
    "            content = self.get_combined_content(proj)\n",
    "            \n",
    "            # Extract only key features (faster)\n",
    "            imports = set(re.findall(r'from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', content)[:100])\n",
    "            functions = set(re.findall(r'(?:function|const)\\s+(\\w+)', content)[:100])\n",
    "            routes = set(re.findall(r'\\.(get|post|put|delete)\\([\\'\"]([^\\'\"]+)', content)[:50])\n",
    "            \n",
    "            all_features[proj] = {\n",
    "                'imports': imports,\n",
    "                'functions': functions,\n",
    "                'routes': routes\n",
    "            }\n",
    "        \n",
    "        # Compute pairwise similarity\n",
    "        for i, proj1 in enumerate(self.project_names):\n",
    "            feat1 = all_features[proj1]\n",
    "            for j in range(i, self.n_projects):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i][j] = 1.0\n",
    "                else:\n",
    "                    proj2 = self.project_names[j]\n",
    "                    feat2 = all_features[proj2]\n",
    "                    \n",
    "                    # Compute Jaccard similarities\n",
    "                    sims = []\n",
    "                    for key in ['imports', 'functions', 'routes']:\n",
    "                        set1, set2 = feat1[key], feat2[key]\n",
    "                        if set1 or set2:\n",
    "                            jaccard = len(set1 & set2) / len(set1 | set2)\n",
    "                            sims.append(jaccard)\n",
    "                    \n",
    "                    avg_sim = np.mean(sims) if sims else 0.0\n",
    "                    similarity_matrix[i][j] = avg_sim\n",
    "                    similarity_matrix[j][i] = avg_sim\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def semantic_similarity_fast(self) -> np.ndarray:\n",
    "        \"\"\"Fast semantic similarity using smaller embeddings\"\"\"\n",
    "        print(\"\\nComputing semantic similarity (fast - using TF-IDF as fallback)...\")\n",
    "        \n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            \n",
    "            # Use smaller chunks\n",
    "            documents = [self.get_combined_content(proj, max_chars=5000) \n",
    "                        for proj in self.project_names]\n",
    "            \n",
    "            embeddings = model.encode(documents, batch_size=8, show_progress_bar=True)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            return similarity_matrix\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic embedding failed: {e}\")\n",
    "            print(\"Using TF-IDF as fallback...\")\n",
    "            return self.textual_similarity_tfidf()\n",
    "\n",
    "# ==================== USAGE EXAMPLE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PROJECTS_DIR = \"ALL_PROJECTS\"\n",
    "    \n",
    "    # Step 1: Preprocessing\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: PREPROCESSING\")\n",
    "    print(\"=\" * 80)\n",
    "    preprocessor = OptimizedCodePreprocessor(PROJECTS_DIR)\n",
    "    summary_df = preprocessor.preprocess_all_projects()\n",
    "    \n",
    "    print(\"\\n=== Project Summary Statistics ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    summary_df.to_csv('results/preprocessing_summary.csv', index=False)\n",
    "    \n",
    "    # Step 2: Similarity Analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: SIMILARITY ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    analyzer = OptimizedSimilarityAnalyzer(preprocessor.projects_data)\n",
    "    \n",
    "    # Use fastest methods\n",
    "    similarity_tfidf = analyzer.textual_similarity_tfidf()\n",
    "    similarity_structural = analyzer.structural_similarity_fast()\n",
    "    \n",
    "    # Optional: Use difflib only if needed (slowest)\n",
    "    # similarity_difflib = analyzer.textual_similarity_difflib_parallel()\n",
    "    \n",
    "    # Optional: Use semantic only if needed\n",
    "    # similarity_semantic = analyzer.semantic_similarity_fast()\n",
    "    \n",
    "    # Save results\n",
    "    np.save('results/similarity_tfidf.npy', similarity_tfidf)\n",
    "    np.save('results/similarity_structural.npy', similarity_structural)\n",
    "    \n",
    "    print(\"\\nâœ… Analysis complete! Results saved to 'results/' folder\")\n",
    "    print(f\"   - Average TF-IDF similarity: {similarity_tfidf[np.triu_indices_from(similarity_tfidf, k=1)].mean():.3f}\")\n",
    "    print(f\"   - Average Structural similarity: {similarity_structural[np.triu_indices_from(similarity_structural, k=1)].mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
