{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd8a119",
   "metadata": {},
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # VidyaVichar Code Similarity Analysis\n",
    "# ## CS6.302 - Assignment 3, Question 1\n",
    "# \n",
    "# This notebook performs comprehensive similarity analysis across 27 MERN stack implementations\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part A: Preprocessing & Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7672ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# ==================== PART A: PREPROCESSING ====================\n",
    "\n",
    "class OptimizedCodePreprocessor:\n",
    "    \"\"\"Optimized preprocessor with all Part A requirements\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_dir: str, max_file_size: int = 100000):\n",
    "        self.projects_dir = Path(projects_dir)\n",
    "        self.valid_extensions = {'.js', '.jsx', '.json', '.css'}\n",
    "        self.projects_data = {}\n",
    "        self.max_file_size = max_file_size\n",
    "        \n",
    "    def remove_comments_and_logs(self, code: str, file_ext: str) -> str:\n",
    "        \"\"\"Remove comments and console logs\"\"\"\n",
    "        if file_ext in ['.js', '.jsx']:\n",
    "            code = re.sub(r'//.*?$', '', code, flags=re.MULTILINE)\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "            code = re.sub(r'console\\.log\\([^)]*\\);?', '', code)\n",
    "        elif file_ext == '.css':\n",
    "            code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "        return code\n",
    "    \n",
    "    def normalize_formatting(self, code: str) -> str:\n",
    "        \"\"\"Normalize whitespace and indentation\"\"\"\n",
    "        lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def is_minified(self, code: str) -> bool:\n",
    "        \"\"\"Check if code is minified\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        if not lines:\n",
    "            return False\n",
    "        avg_line_length = sum(len(line) for line in lines) / len(lines)\n",
    "        return avg_line_length > 200\n",
    "    \n",
    "    def preprocess_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Preprocess a single file\"\"\"\n",
    "        try:\n",
    "            if file_path.stat().st_size > self.max_file_size:\n",
    "                return \"\"\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            if self.is_minified(content):\n",
    "                return \"\"\n",
    "            \n",
    "            content = self.remove_comments_and_logs(content, file_path.suffix)\n",
    "            content = self.normalize_formatting(content)\n",
    "            return content\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_project(self, project_path: Path) -> Dict:\n",
    "        \"\"\"Analyze project - Part A requirements\"\"\"\n",
    "        stats = {\n",
    "            'project_name': project_path.name,\n",
    "            'total_files': 0,\n",
    "            'total_folders': 0,\n",
    "            'loc': 0,\n",
    "            'js_files': 0,\n",
    "            'jsx_files': 0,\n",
    "            'json_files': 0,\n",
    "            'css_files': 0,\n",
    "            'react_components': 0,\n",
    "            'express_routes': 0,\n",
    "            'mongoose_models': 0,\n",
    "            'file_contents': {},\n",
    "            'combined_content': \"\"\n",
    "        }\n",
    "        \n",
    "        stats['total_folders'] = sum(1 for _ in project_path.rglob('*') if _.is_dir())\n",
    "        \n",
    "        valid_files = [f for f in project_path.rglob('*') \n",
    "                      if f.is_file() and f.suffix in self.valid_extensions]\n",
    "        \n",
    "        all_content = []\n",
    "        for file_path in valid_files:\n",
    "            stats['total_files'] += 1\n",
    "            ext = file_path.suffix\n",
    "            \n",
    "            if ext == '.js':\n",
    "                stats['js_files'] += 1\n",
    "            elif ext == '.jsx':\n",
    "                stats['jsx_files'] += 1\n",
    "            elif ext == '.json':\n",
    "                stats['json_files'] += 1\n",
    "            elif ext == '.css':\n",
    "                stats['css_files'] += 1\n",
    "            \n",
    "            content = self.preprocess_file(file_path)\n",
    "            if content:\n",
    "                relative_path = str(file_path.relative_to(project_path))\n",
    "                stats['file_contents'][relative_path] = content\n",
    "                all_content.append(content)\n",
    "                stats['loc'] += len(content.split('\\n'))\n",
    "                \n",
    "                if ext in ['.js', '.jsx']:\n",
    "                    if 'React.Component' in content or 'return (' in content:\n",
    "                        stats['react_components'] += 1\n",
    "                    if re.search(r'(app|router)\\.(get|post|put|delete)', content):\n",
    "                        stats['express_routes'] += 1\n",
    "                    if 'mongoose.model' in content or 'mongoose.Schema' in content:\n",
    "                        stats['mongoose_models'] += 1\n",
    "        \n",
    "        stats['combined_content'] = '\\n\\n'.join(all_content)\n",
    "        return stats\n",
    "    \n",
    "    def preprocess_all_projects(self) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess all projects\"\"\"\n",
    "        all_stats = []\n",
    "        project_dirs = [d for d in self.projects_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        print(f\"Found {len(project_dirs)} projects to analyze...\")\n",
    "        \n",
    "        for i, project_dir in enumerate(project_dirs, 1):\n",
    "            print(f\"Processing {i}/{len(project_dirs)}: {project_dir.name}\")\n",
    "            stats = self.analyze_project(project_dir)\n",
    "            self.projects_data[project_dir.name] = stats\n",
    "            all_stats.append({\n",
    "                'Project': stats['project_name'],\n",
    "                'Total Files': stats['total_files'],\n",
    "                'Total Folders': stats['total_folders'],\n",
    "                'Lines of Code': stats['loc'],\n",
    "                'JS Files': stats['js_files'],\n",
    "                'JSX Files': stats['jsx_files'],\n",
    "                'JSON Files': stats['json_files'],\n",
    "                'CSS Files': stats['css_files'],\n",
    "                'React Components': stats['react_components'],\n",
    "                'Express Routes': stats['express_routes'],\n",
    "                'Mongoose Models': stats['mongoose_models']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(all_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6da01",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Part B: Code Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67fc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difflib_pair(args):\n",
    "    \"\"\"Compute difflib similarity for parallel processing\"\"\"\n",
    "    i, j, content1, content2 = args\n",
    "    if i == j:\n",
    "        return (i, j, 1.0)\n",
    "    matcher = SequenceMatcher(None, content1, content2)\n",
    "    quick = matcher.quick_ratio()\n",
    "    ratio = matcher.ratio() if quick > 0.5 else quick * 0.9\n",
    "    return (i, j, ratio)\n",
    "\n",
    "class OptimizedSimilarityAnalyzer:\n",
    "    \"\"\"Part B: Multi-level similarity analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, projects_data: Dict):\n",
    "        self.projects_data = projects_data\n",
    "        self.project_names = list(projects_data.keys())\n",
    "        self.n_projects = len(self.project_names)\n",
    "        \n",
    "    def get_combined_content(self, project_name: str, max_chars: int = None) -> str:\n",
    "        \"\"\"Get combined content with optional truncation\"\"\"\n",
    "        content = self.projects_data[project_name]['combined_content']\n",
    "        if max_chars and len(content) > max_chars:\n",
    "            step = len(content) // 5\n",
    "            samples = [content[i:i+max_chars//5] for i in range(0, len(content), step)][:5]\n",
    "            return '\\n'.join(samples)\n",
    "        return content\n",
    "    \n",
    "    def textual_similarity_tfidf(self) -> np.ndarray:\n",
    "        \"\"\"Level 1: Textual Similarity using TF-IDF\"\"\"\n",
    "        print(\"\\n[1/3] Computing Textual Similarity (TF-IDF + Cosine)...\")\n",
    "        \n",
    "        documents = [self.get_combined_content(proj, max_chars=100000) \n",
    "                    for proj in self.project_names]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=3000,\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            ngram_range=(1, 2),\n",
    "            max_df=0.95,\n",
    "            min_df=2\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        return cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    def structural_similarity(self) -> np.ndarray:\n",
    "        \"\"\"Level 2: Structural Similarity (AST-like features)\"\"\"\n",
    "        print(\"[2/3] Computing Structural Similarity (AST features)...\")\n",
    "        similarity_matrix = np.zeros((self.n_projects, self.n_projects))\n",
    "        \n",
    "        # Extract structural features\n",
    "        all_features = {}\n",
    "        for proj in self.project_names:\n",
    "            content = self.get_combined_content(proj)\n",
    "            all_features[proj] = {\n",
    "                'imports': set(re.findall(r'from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', content)[:100]),\n",
    "                'functions': set(re.findall(r'(?:function|const)\\s+(\\w+)', content)[:100]),\n",
    "                'routes': set(re.findall(r'\\.(get|post|put|delete)\\([\\'\"]([^\\'\"]+)', content)[:50])\n",
    "            }\n",
    "        \n",
    "        # Compute Jaccard similarity\n",
    "        for i, proj1 in enumerate(self.project_names):\n",
    "            feat1 = all_features[proj1]\n",
    "            for j in range(i, self.n_projects):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i][j] = 1.0\n",
    "                else:\n",
    "                    proj2 = self.project_names[j]\n",
    "                    feat2 = all_features[proj2]\n",
    "                    \n",
    "                    sims = []\n",
    "                    for key in ['imports', 'functions', 'routes']:\n",
    "                        set1, set2 = feat1[key], feat2[key]\n",
    "                        if set1 or set2:\n",
    "                            sims.append(len(set1 & set2) / len(set1 | set2))\n",
    "                    \n",
    "                    avg_sim = np.mean(sims) if sims else 0.0\n",
    "                    similarity_matrix[i][j] = avg_sim\n",
    "                    similarity_matrix[j][i] = avg_sim\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def semantic_similarity(self) -> np.ndarray:\n",
    "        \"\"\"Level 3: Semantic Similarity using embeddings\"\"\"\n",
    "        print(\"[3/3] Computing Semantic Similarity (Embeddings)...\")\n",
    "        \n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            documents = [self.get_combined_content(proj, max_chars=5000) \n",
    "                        for proj in self.project_names]\n",
    "            embeddings = model.encode(documents, batch_size=8, show_progress_bar=False)\n",
    "            return cosine_similarity(embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Semantic embedding failed ({e}). Using TF-IDF fallback.\")\n",
    "            return self.textual_similarity_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52457d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Part C: Visualization & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fcf1124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART A: PREPROCESSING & DATA UNDERSTANDING\n",
      "================================================================================\n",
      "Found 18 projects to analyze...\n",
      "Processing 1/18: Team-26\n",
      "Processing 2/18: Team-03\n",
      "Processing 3/18: Team-27\n",
      "Processing 4/18: Team-33\n",
      "Processing 5/18: Team-28\n",
      "Processing 6/18: Team-06\n",
      "Processing 7/18: Team-29\n",
      "Processing 8/18: Team-31\n",
      "Processing 9/18: Team-16\n",
      "Processing 10/18: Team-22\n",
      "Processing 11/18: Team-25\n",
      "Processing 12/18: Team-05\n",
      "Processing 13/18: Team-20\n",
      "Processing 14/18: Team-10\n",
      "Processing 15/18: Team-14\n",
      "Processing 16/18: Team-21\n",
      "Processing 17/18: Team-13\n",
      "Processing 18/18: Team-32\n",
      "\n",
      "=== Project Summary Statistics ===\n",
      "Project  Total Files  Total Folders  Lines of Code  JS Files  JSX Files  JSON Files  CSS Files  React Components  Express Routes  Mongoose Models\n",
      "Team-26           29             27           4429        16          7           4          2                 7               5                4\n",
      "Team-03           32             30           3513        12         12           6          2                10               2                2\n",
      "Team-27           53             35           5434        39         10           3          1                 9               7                3\n",
      "Team-33           46             32           2495        31          0           3         12                14               4                3\n",
      "Team-28           37             30           3987        30          0           5          2                11               3                3\n",
      "Team-06           44             30           3177        20         19           3          2                17               2                4\n",
      "Team-29           36             31           2335        27          0           7          2                 8               2                3\n",
      "Team-31           23             28            924         9          6           2          6                 5               2                1\n",
      "Team-16           53             39           5182        17         26           8          2                25               6                3\n",
      "Team-22           51             35           6343        44          0           5          2                17               4                4\n",
      "Team-25           42             32           4554        17         20           4          1                18               4                3\n",
      "Team-05           53             31           4269        16         29           4          4                29               3                6\n",
      "Team-20           46             35           5013        24         16           5          1                15               2                4\n",
      "Team-10           30             30           6258        14         10           4          2                10               4                3\n",
      "Team-14           42             29           3222        20         11           4          7                10               4                3\n",
      "Team-21           23             28           1216        20          0           3          0                 5               4                3\n",
      "Team-13         1129            406         111600       919         14         194          2               119               4                5\n",
      "Team-32           32             25           4123        25          0           5          2                13               3                3\n",
      "\n",
      "       Total Files  Total Folders  Lines of Code    JS Files  JSX Files  JSON Files  CSS Files  React Components  Express Routes  Mongoose Models\n",
      "count    18.000000      18.000000       18.00000   18.000000  18.000000   18.000000  18.000000         18.000000       18.000000        18.000000\n",
      "mean    100.055556      51.833333     9893.00000   72.222222  10.000000   14.944444   2.888889         19.000000        3.611111         3.333333\n",
      "std     256.982140      88.452214    25428.19401  211.525428   9.330532   44.711457   2.846854         25.765915        1.419979         1.084652\n",
      "min      23.000000      25.000000      924.00000    9.000000   0.000000    2.000000   0.000000          5.000000        2.000000         1.000000\n",
      "25%      32.000000      29.250000     3188.25000   16.250000   0.000000    3.250000   2.000000          9.250000        2.250000         3.000000\n",
      "50%      42.000000      30.500000     4196.00000   20.000000  10.000000    4.000000   2.000000         12.000000        4.000000         3.000000\n",
      "75%      49.750000      34.250000     5139.75000   29.250000  15.500000    5.000000   2.000000         17.000000        4.000000         4.000000\n",
      "max    1129.000000     406.000000   111600.00000  919.000000  29.000000  194.000000  12.000000        119.000000        7.000000         6.000000\n",
      "\n",
      "✓ Saved: results/preprocessing_summary.csv\n",
      "\n",
      "================================================================================\n",
      "PART B: CODE SIMILARITY COMPUTATION\n",
      "================================================================================\n",
      "\n",
      "[1/3] Computing Textual Similarity (TF-IDF + Cosine)...\n",
      "[2/3] Computing Structural Similarity (AST features)...\n",
      "[3/3] Computing Semantic Similarity (Embeddings)...\n",
      "\n",
      "✓ All similarity matrices saved\n",
      "\n",
      "================================================================================\n",
      "PART C: VISUALIZATION & REPORTING\n",
      "================================================================================\n",
      "\n",
      "Generating visualizations...\n",
      "  ✓ Saved: results/heatmap_textual.png\n",
      "  ✓ Saved: results/heatmap_structural.png\n",
      "  ✓ Saved: results/heatmap_semantic.png\n",
      "  ✓ Saved: results/network_textual.png\n",
      "  ✓ Saved: results/network_structural.png\n",
      "  ✓ Saved: results/average_similarities_by_metric.png\n",
      "  ✓ Saved: results/distribution_textual.png\n",
      "  ✓ Saved: results/distribution_structural.png\n",
      "\n",
      "================================================================================\n",
      "SIMILARITY ANALYSIS RESULTS\n",
      "================================================================================\n",
      "\n",
      "Textual (TF-IDF) Similarity:\n",
      "\n",
      "  Top 5 Most Similar Pairs:\n",
      "    • Team-28 ↔ Team-29: 0.9274\n",
      "    • Team-28 ↔ Team-20: 0.8905\n",
      "    • Team-28 ↔ Team-32: 0.8905\n",
      "    • Team-29 ↔ Team-20: 0.8829\n",
      "    • Team-29 ↔ Team-32: 0.8820\n",
      "\n",
      "  Top 5 Least Similar Pairs:\n",
      "    • Team-26 ↔ Team-05: 0.0799\n",
      "    • Team-22 ↔ Team-13: 0.0758\n",
      "    • Team-06 ↔ Team-13: 0.0719\n",
      "    • Team-26 ↔ Team-10: 0.0546\n",
      "    • Team-26 ↔ Team-22: 0.0472\n",
      "\n",
      "Structural (AST) Similarity:\n",
      "\n",
      "  Top 5 Most Similar Pairs:\n",
      "    • Team-33 ↔ Team-28: 0.1919\n",
      "    • Team-31 ↔ Team-16: 0.1847\n",
      "    • Team-33 ↔ Team-32: 0.1769\n",
      "    • Team-33 ↔ Team-29: 0.1748\n",
      "    • Team-03 ↔ Team-31: 0.1733\n",
      "\n",
      "  Top 5 Least Similar Pairs:\n",
      "    • Team-20 ↔ Team-21: 0.0283\n",
      "    • Team-16 ↔ Team-13: 0.0268\n",
      "    • Team-22 ↔ Team-14: 0.0266\n",
      "    • Team-06 ↔ Team-22: 0.0245\n",
      "    • Team-10 ↔ Team-13: 0.0204\n",
      "\n",
      "Semantic (Embeddings) Similarity:\n",
      "\n",
      "  Top 5 Most Similar Pairs:\n",
      "    • Team-28 ↔ Team-32: 1.0000\n",
      "    • Team-33 ↔ Team-28: 0.9991\n",
      "    • Team-33 ↔ Team-32: 0.9991\n",
      "    • Team-28 ↔ Team-22: 0.9831\n",
      "    • Team-22 ↔ Team-32: 0.9831\n",
      "\n",
      "  Top 5 Least Similar Pairs:\n",
      "    • Team-28 ↔ Team-31: 0.2552\n",
      "    • Team-31 ↔ Team-32: 0.2552\n",
      "    • Team-31 ↔ Team-22: 0.2330\n",
      "    • Team-26 ↔ Team-10: 0.2302\n",
      "    • Team-31 ↔ Team-10: 0.0685\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "               Metric     Mean   Median  Std Dev      Min      Max\n",
      "     Textual (TF-IDF) 0.325606 0.245751 0.236361 0.047182 0.927447\n",
      "     Structural (AST) 0.081923 0.072594 0.038403 0.020394 0.191867\n",
      "Semantic (Embeddings) 0.559560 0.551141 0.193373 0.068513 1.000000\n",
      "\n",
      "  ✓ Saved: results/summary_statistics.csv\n",
      "\n",
      "================================================================================\n",
      "✅ ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files in results/:\n",
      "  • preprocessing_summary.csv\n",
      "  • similarity_textual.csv/npy\n",
      "  • similarity_structural.csv/npy\n",
      "  • similarity_semantic.csv/npy\n",
      "  • heatmap_textual.png\n",
      "  • heatmap_structural.png\n",
      "  • heatmap_semantic.png\n",
      "  • network_textual.png\n",
      "  • network_structural.png\n",
      "  • average_similarities_by_metric.png\n",
      "  • distribution_textual.png\n",
      "  • distribution_structural.png\n",
      "  • summary_statistics.csv\n",
      "\n",
      "Next: Write your 1-page report.pdf with methodology and insights!\n"
     ]
    }
   ],
   "source": [
    "class SimilarityVisualizer:\n",
    "    \"\"\"Part C: Comprehensive visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, project_names: List[str]):\n",
    "        self.project_names = project_names\n",
    "        plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        \n",
    "    def plot_heatmap(self, similarity_matrix: np.ndarray, title: str, filename: str):\n",
    "        \"\"\"Heatmap of similarity matrix\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        \n",
    "        sns.heatmap(\n",
    "            similarity_matrix,\n",
    "            xticklabels=self.project_names,\n",
    "            yticklabels=self.project_names,\n",
    "            annot=False,\n",
    "            fmt='.2f',\n",
    "            cmap='YlOrRd',\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            cbar_kws={'label': 'Similarity Score'},\n",
    "            square=True,\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Projects', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Projects', fontsize=12, fontweight='bold')\n",
    "        plt.xticks(rotation=90, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_network_graph(self, similarity_matrix: np.ndarray, title: str, \n",
    "                          filename: str, threshold: float = 0.3):\n",
    "        \"\"\"Network graph of project clusters\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        for name in self.project_names:\n",
    "            G.add_node(name)\n",
    "        \n",
    "        for i in range(len(self.project_names)):\n",
    "            for j in range(i + 1, len(self.project_names)):\n",
    "                if similarity_matrix[i][j] >= threshold:\n",
    "                    G.add_edge(\n",
    "                        self.project_names[i],\n",
    "                        self.project_names[j],\n",
    "                        weight=similarity_matrix[i][j]\n",
    "                    )\n",
    "        \n",
    "        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "        \n",
    "        # Node colors based on degree\n",
    "        degrees = dict(G.degree())\n",
    "        node_colors = [degrees.get(node, 0) for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_size=700, \n",
    "                              node_color=node_colors, cmap='viridis',\n",
    "                              alpha=0.9, ax=ax)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold', ax=ax)\n",
    "        \n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] for u, v in edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=[w*4 for w in weights], \n",
    "                              alpha=0.5, edge_color='gray', ax=ax)\n",
    "        \n",
    "        ax.set_title(f'{title}\\n(Threshold: {threshold:.2f}, Edges: {len(edges)})', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_bar_chart(self, matrices: Dict[str, np.ndarray], filename: str):\n",
    "        \"\"\"Bar chart of average similarities by metric\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        metric_names = list(matrices.keys())\n",
    "        avg_similarities = []\n",
    "        std_similarities = []\n",
    "        \n",
    "        for matrix in matrices.values():\n",
    "            mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "            values = matrix[mask]\n",
    "            avg_similarities.append(values.mean())\n",
    "            std_similarities.append(values.std())\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        bars = ax.bar(metric_names, avg_similarities, color=colors, \n",
    "                     yerr=std_similarities, capsize=10, alpha=0.8, \n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        ax.set_ylabel('Average Similarity Score', fontsize=13, fontweight='bold')\n",
    "        ax.set_xlabel('Similarity Metric', fontsize=13, fontweight='bold')\n",
    "        ax.set_title('Average Similarity Scores by Metric', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        for bar, val, std in zip(bars, avg_similarities, std_similarities):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,\n",
    "                   f'{val:.3f}±{std:.3f}',\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved: results/{filename}\")\n",
    "    \n",
    "    def plot_distribution(self, similarity_matrix: np.ndarray, title: str, filename: str):\n",
    "        \"\"\"Distribution histogram of similarity scores\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "        similarities = similarity_matrix[mask]\n",
    "        \n",
    "        ax.hist(similarities, bins=30, color='steelblue', alpha=0.7, \n",
    "               edgecolor='black', linewidth=1.2)\n",
    "        ax.axvline(similarities.mean(), color='red', linestyle='--', \n",
    "                  linewidth=2.5, label=f'Mean: {similarities.mean():.3f}')\n",
    "        ax.axvline(np.median(similarities), color='green', linestyle='--', \n",
    "                  linewidth=2.5, label=f'Median: {np.median(similarities):.3f}')\n",
    "        \n",
    "        ax.set_xlabel('Similarity Score', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "        ax.legend(fontsize=11, loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved: results/{filename}\")\n",
    "\n",
    "# ==================== ANALYSIS & REPORTING ====================\n",
    "\n",
    "def analyze_similarities(matrices: Dict, project_names: List[str]):\n",
    "    \"\"\"Generate detailed analysis for report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SIMILARITY ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for metric_name, matrix in matrices.items():\n",
    "        print(f\"\\n{metric_name} Similarity:\")\n",
    "        \n",
    "        # Find most and least similar pairs\n",
    "        n = len(project_names)\n",
    "        pairs = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                pairs.append((project_names[i], project_names[j], matrix[i][j]))\n",
    "        \n",
    "        pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(\"\\n  Top 5 Most Similar Pairs:\")\n",
    "        for proj1, proj2, sim in pairs[:5]:\n",
    "            print(f\"    • {proj1} ↔ {proj2}: {sim:.4f}\")\n",
    "        \n",
    "        print(\"\\n  Top 5 Least Similar Pairs:\")\n",
    "        for proj1, proj2, sim in pairs[-5:]:\n",
    "            print(f\"    • {proj1} ↔ {proj2}: {sim:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = {\n",
    "        'Metric': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'Std Dev': [],\n",
    "        'Min': [],\n",
    "        'Max': []\n",
    "    }\n",
    "    \n",
    "    for metric_name, matrix in matrices.items():\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        values = matrix[mask]\n",
    "        \n",
    "        summary_data['Metric'].append(metric_name)\n",
    "        summary_data['Mean'].append(values.mean())\n",
    "        summary_data['Median'].append(np.median(values))\n",
    "        summary_data['Std Dev'].append(values.std())\n",
    "        summary_data['Min'].append(values.min())\n",
    "        summary_data['Max'].append(values.max())\n",
    "    \n",
    "    stats_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + stats_df.to_string(index=False))\n",
    "    stats_df.to_csv('results/summary_statistics.csv', index=False)\n",
    "    print(\"\\n  ✓ Saved: results/summary_statistics.csv\")\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Complete analysis pipeline\"\"\"\n",
    "    PROJECTS_DIR = \"ALL_PROJECTS\"\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PART A: PREPROCESSING & DATA UNDERSTANDING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Preprocessing\n",
    "    preprocessor = OptimizedCodePreprocessor(PROJECTS_DIR)\n",
    "    summary_df = preprocessor.preprocess_all_projects()\n",
    "    \n",
    "    print(\"\\n=== Project Summary Statistics ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"\\n\" + summary_df.describe().to_string())\n",
    "    \n",
    "    summary_df.to_csv('results/preprocessing_summary.csv', index=False)\n",
    "    print(\"\\n✓ Saved: results/preprocessing_summary.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PART B: CODE SIMILARITY COMPUTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 2: Similarity Analysis\n",
    "    analyzer = OptimizedSimilarityAnalyzer(preprocessor.projects_data)\n",
    "    \n",
    "    similarity_textual = analyzer.textual_similarity_tfidf()\n",
    "    similarity_structural = analyzer.structural_similarity()\n",
    "    similarity_semantic = analyzer.semantic_similarity()\n",
    "    \n",
    "    # Save matrices\n",
    "    np.save('results/similarity_textual.npy', similarity_textual)\n",
    "    np.save('results/similarity_structural.npy', similarity_structural)\n",
    "    np.save('results/similarity_semantic.npy', similarity_semantic)\n",
    "    \n",
    "    # Save as CSV\n",
    "    project_names = analyzer.project_names\n",
    "    pd.DataFrame(similarity_textual, index=project_names, columns=project_names).to_csv('results/similarity_textual.csv')\n",
    "    pd.DataFrame(similarity_structural, index=project_names, columns=project_names).to_csv('results/similarity_structural.csv')\n",
    "    pd.DataFrame(similarity_semantic, index=project_names, columns=project_names).to_csv('results/similarity_semantic.csv')\n",
    "    \n",
    "    print(\"\\n✓ All similarity matrices saved\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PART C: VISUALIZATION & REPORTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 3: Visualizations\n",
    "    visualizer = SimilarityVisualizer(project_names)\n",
    "    \n",
    "    matrices_dict = {\n",
    "        'Textual (TF-IDF)': similarity_textual,\n",
    "        'Structural (AST)': similarity_structural,\n",
    "        'Semantic (Embeddings)': similarity_semantic\n",
    "    }\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # Heatmaps (required)\n",
    "    visualizer.plot_heatmap(similarity_textual, 'Textual Similarity Heatmap (TF-IDF)', 'heatmap_textual.png')\n",
    "    visualizer.plot_heatmap(similarity_structural, 'Structural Similarity Heatmap (AST)', 'heatmap_structural.png')\n",
    "    visualizer.plot_heatmap(similarity_semantic, 'Semantic Similarity Heatmap (Embeddings)', 'heatmap_semantic.png')\n",
    "    \n",
    "    # Network graphs (required)\n",
    "    visualizer.plot_network_graph(similarity_textual, 'Project Similarity Network (Textual)', 'network_textual.png', 0.3)\n",
    "    visualizer.plot_network_graph(similarity_structural, 'Project Similarity Network (Structural)', 'network_structural.png', 0.3)\n",
    "    \n",
    "    # Bar chart (required)\n",
    "    visualizer.plot_bar_chart(matrices_dict, 'average_similarities_by_metric.png')\n",
    "    \n",
    "    # Additional visualizations\n",
    "    visualizer.plot_distribution(similarity_textual, 'Distribution of Textual Similarity Scores', 'distribution_textual.png')\n",
    "    visualizer.plot_distribution(similarity_structural, 'Distribution of Structural Similarity Scores', 'distribution_structural.png')\n",
    "    \n",
    "    # Step 4: Analysis\n",
    "    analyze_similarities(matrices_dict, project_names)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files in results/:\")\n",
    "    print(\"  • preprocessing_summary.csv\")\n",
    "    print(\"  • similarity_textual.csv/npy\")\n",
    "    print(\"  • similarity_structural.csv/npy\")\n",
    "    print(\"  • similarity_semantic.csv/npy\")\n",
    "    print(\"  • heatmap_textual.png\")\n",
    "    print(\"  • heatmap_structural.png\")\n",
    "    print(\"  • heatmap_semantic.png\")\n",
    "    print(\"  • network_textual.png\")\n",
    "    print(\"  • network_structural.png\")\n",
    "    print(\"  • average_similarities_by_metric.png\")\n",
    "    print(\"  • distribution_textual.png\")\n",
    "    print(\"  • distribution_structural.png\")\n",
    "    print(\"  • summary_statistics.csv\")\n",
    "    print(\"\\nNext: Write your 1-page report.pdf with methodology and insights!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2e351-ab16-44fa-aac2-eca78845968f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56d050-a599-40b1-8b88-05a6153827ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-env",
   "language": "python",
   "name": "similarity-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
